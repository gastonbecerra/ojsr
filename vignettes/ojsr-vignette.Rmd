---
title: "ojsr-vignette"
author: "Gaston Becerra"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{ojsr-vignette}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup, include = FALSE}
library(knitr)
```



# Overview



The aim of this package is to aid retrieve/scraping meta-data and/or accessing galleys (full-text and supplementary files) from OJS url(s).

This package currently provides:

- 1 function (**process_urls**) to parse OJS ulrs, according to OJS routing conventions, to identify what type of page are they pointing at, and to rewrite the expected url for each scrapping task;
- 1 function (**get_metadata_from_page**) to collect meta-data from OJS articles page (via html scraping)
- 1 function (**get_galley_urls**) to retrieve meta-data from OJS article OAI record

It is intended that this package will include:

- 1 function to crawl archive
- 1 function to crawl issue(s)
- 1 function to crawl search results
- some auxiliary functions (to identify OJS version (via generator metatag), to store/download full-text files, etc.)
- 1 dataset of handy codes for scraping


## Important! 

Please keep in mind:

- **Most of ojsr functions rely heavily on OJS conventions and default themes**, and therefore are most likely to fail when used on customized OJS installments (i.e., customized themes, routing "improvements" via .htaccess) and non-compliant OJS configuration and record keeping (i.e., when metadata has not been well-broomed by the Editor/Publisher, or OAI records do not follow standards). 
- Also, please notice that only **common OJS v1 and v2 convensions** are considered (see "OJS v3 and OJS API" below).



## What's OJS?

(from the OJS documentation <https://pkp.sfu.ca/ojs/>, as of Jan.2020)

Open Journal Systems (OJS) is a journal management and publishing system that has been developed by the Public Knowledge Project through its federally funded efforts to expand and improve access to research.

OJS assists with every stage of the refereed publishing process, from submissions through to online publication and indexing. Through its management systems, its finely grained indexing of research, and the context it provides for research, OJS seeks to improve both the scholarly and public quality of refereed research.

OJS is open source software made freely available to journals worldwide for the purpose of making open access publishing a viable option for more journals, as open access can increase a journalâ€™s readership as well as its contribution to the public good on a global scale (see PKP Publications).

## OJS v3 and OJS API

There is not a simpler, more reliable (less conventional!), way to access meta-data from OJS? 

Yes, there is! Since OJS v3.1+ <https://docs.pkp.sfu.ca/dev/api/ojs/3.1> a Rest API is provided, and we're positive a better R interface should use that API instead of web scraping themes and OAI records.

So, why ojsr? 

Well... According to <https://pkp.sfu.ca/ojs/ojs-usage/ojs-stats/>, as of 2019 (when v3.1+ was launched), OJS was being used by at least 10,000 journals worldwide. OJS is an excellent free publishing solution for institutions that could not publish otherwise. Presumably, most of them do not have the the technical nor financial capabilities to update constantly, so OJS v1 and v2 are expected to be around for quite some time... 



# How to use ojsr?



## process_urls: Parse urls to check what type of OJS page they point to

Let's say you have a bunch of OJS specific article urls you'd like to scrap their meta-data, or a curated list of journals you'd like to crawl completely. However, not all of them may be poiting to the *right* type of OJS page for your purposes (e.g., articles may be pointing at the PDF galley to download wherein you can not scrap meta-data, or the issues may be pointing at cover pages that do not include ToCs). 

`process_urls()` allows you to parse a list of urls against OJS **routing conventions** to determine what type of page are they pointing at, which parameters are invoked (e.g., article, issues, galley IDs). 

Also, it will generate the *(assumed to be) right* url for specific purposes, such as scraping meta-data, or scraping the list of issues for the archive of issues page. That's why `process_urls()` can be invoked before the scraping functions if you are not sure about the list of urls you already have (see the example code for `get_issue_urls_from_archive()`). 

Please keep in mind that since we are working with **routing conventions** (see <https://docs.pkp.sfu.ca/dev/documentation/en/architecture-routes>), these pages are not actually browsed.

Your urls could look like these:

```{r}

url_sample <- c( 'https://publicaciones.sociales.uba.ar/index.php/psicologiasocial/article/view/2903',
          'http://fundacionmenteclara.org.ar/revista/index.php/RCA/issue/view/2018-Vol3-2',
          'http://fundacionmenteclara.org.ar/revista/index.php/RCA/article/download/43/54',
          'https://firstmonday.org/ojs/index.php/fm/article/view/9540',
          'http://imed.pub/ojs/index.php/iam/article/view/1650',
          'http://fundacionmenteclara.org.ar/revista/index.php/RCA/oai',
          'https://publicaciones.sociales.uba.ar/index.php/psicologiasocial/issue/view/444',
          'https://publicaciones.sociales.uba.ar/index.php/psicologiasocial/search/search?query=complejidad'
)

```

We'll parse them, and store the result in a dataframes (`what_type_of_urls`)

```{r message=FALSE}

library(tidyverse) # we'll use dplyr and tidy several times in the vignette
library(ojsr) 
what_type_of_urls <- ojsr::process_urls(url_sample)
glimpse(what_type_of_urls)

```

The resulting dataframe contains the following:

1. *url* - the url provided
2. *page* - the page class the url is pointing at (article, issue, oai, search, etc.)
3. *command* - the name of the function within the page (view, download, search, etc.)
4. *expect* - what you could do at this page; usually a combination of command + page or a variant (view article, download galley)
5. 

- *article_abstract* urls (i.e., <https://papiro.unizar.es/ojs/index.php/rc51-jos/article/view/1047>) pointing at a page with the title of the article, the abstract, and the links to "galleys" (full-content text) and supplementary materials
- *download_galley* urls (i.e., <https://papiro.unizar.es/ojs/index.php/rc51-jos/article/view/1047/1050>) pointing at full-content in pdf, html, epub, etc. This may or may not force download
- *issue* urls (i.e., <https://papiro.unizar.es/ojs/index.php/rc51-jos/issue/view/91>) pointing at a full issue of a journal
- *oai* urls (i.e., <https://papiro.unizar.es/ojs/index.php/rc51-jos/oai>) pointing at the entry page of the oai protocol


## get_issue_urls_from_archive: Retrieve issues urls from issue/archive pages

Let's say you want to crawl a few journals in order to list their issues, so you can latter iterate through them and scrap their articles' meta-data. We can use `get_issue_urls_from_archive()` for that. 

`get_issue_urls_from_archive()` requires a list OJS urls pointing at the archive of issues of each journal (e.g., <https://papiro.unizar.es/ojs/index.php/rc51-jos/issue/archive>). 

However, if you provide the url of any pages of the OJS, `get_issue_urls_from_archive()` can take the *assumed-to-be-right* url, by calling to `process_urls()` internally. In the following example, we are providing a short list of OJS urls, mixing both the archive of issues and home pages.

```{r}

# let's start by providing a mix

socPsy_urls <- c( # a few argentinian social psychology journals 
  'http://sportsem.uv.es/j_sports_and_em/index.php/rips/', # home page of journal, including ToC of current issue
  'https://dspace.palermo.edu/ojs/index.php/psicodebate/issue/archive', # archive of issues
  'https://publicaciones.sociales.uba.ar/index.php/psicologiasocial/issue/view/526' # a particular issue's cover page (no ToC)
)

# let's use get_issue_urls_from_archive to retrieve the issues url from the archive of issue page of these OJS
# we'll allow get_issue_urls_from_archive to use the assummed url 



```
