---
title: "ojsr-vignette"
author: "Gaston Becerra"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{ojsr-vignette}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup, include = FALSE}
#library(ojsr)
library(knitr)
```



# Overview



The aim of this package is to aid retrieve/scraping meta-data and/or accessing galleys (full-text and supplementary files) from OJS url(s).

This package currently provides:

- 1 function (**process_urls**) to parse OJS ulrs, according to OJS routing conventions, to identify what type of page are they pointing to;
- 1 function (**get_metadata_from_page**) to collect meta-data from OJS articles page (via html scraping)
- 1 function (**get_galley_urls**) to retrieve meta-data from OJS article OAI record

It is intended that this package will include:

- 1 function to crawl archive and issue(s)
- some auxiliary functions (to identify OJS version (via generator metatag), to store/download full-text files, etc.)
- 1 dataset of handy codes for scraping


## Important! 

Please keep in mind:

- **Most of ojsr functions rely heavily on OJS conventions and default themes**, and therefore are most likely to fail when used on customized OJS installments (i.e., customized themes, routing "improvements" via .htaccess) and non-compliant OJS configuration and record keeping (i.e., when metadata has not been well-broomed by the Editor/Publisher, or OAI records do not follow standards). 
- Also, please notice that only **common OJS v1 and v2 convensions** are considered (see "OJS v3 and OJS API" below).



## What's OJS?

(from the OJS documentation <https://pkp.sfu.ca/ojs/>, as of Jan.2020)

Open Journal Systems (OJS) is a journal management and publishing system that has been developed by the Public Knowledge Project through its federally funded efforts to expand and improve access to research.

OJS assists with every stage of the refereed publishing process, from submissions through to online publication and indexing. Through its management systems, its finely grained indexing of research, and the context it provides for research, OJS seeks to improve both the scholarly and public quality of refereed research.

OJS is open source software made freely available to journals worldwide for the purpose of making open access publishing a viable option for more journals, as open access can increase a journalâ€™s readership as well as its contribution to the public good on a global scale (see PKP Publications).

## OJS v3 and OJS API

There is not a simpler, more reliable (less conventional!), way to access meta-data from OJS? Yes, there is! Since OJS v3.1+ <https://docs.pkp.sfu.ca/dev/api/ojs/3.1> a Rest API is provided, and we're positive a better R interface should use that API instead of web scraping themes and OAI records.

So, why ojsr? Well... According to <https://pkp.sfu.ca/ojs/ojs-usage/ojs-stats/>, as of 2019 (when v3.1+ was launched), OJS was being used by at least 10,000 journals worldwide. OJS is an excellent free publishing solution for institutions that could not publish otherwise. Presumably, most of them do not have the the technical nor financial capabilities to update constantly, so OJS v1 and v2 are expected to be around for quite some time... 



# How to use ojsr?



## process_urls: Parse OJS urls to predict what type of page they point to

The most likely case scenario is that you have a bunch of article urls, probably imported from the results of a bibliographic query, and want to get the articles meta-data or the full-context. However, since you can do different tasks with these (e.g., you can not scrap metadata from pdf galleys), you first need to know what type of pages are your urls referring to. 

You may use `process_urls()` to parse a list of urls against OJS *routing conventions* (pages are visited actually!) to determine their type. 

ojsr recognizes 4 types of urls:

- *article_abstract* urls (i.e., <https://papiro.unizar.es/ojs/index.php/rc51-jos/article/view/1047>) pointing to a page with the title of the article, the abstract, and the links to "galleys" (full-content text) and supplementary materials
- *download_galley* urls (i.e., <https://papiro.unizar.es/ojs/index.php/rc51-jos/article/view/1047/1050>) pointing to full-content in pdf, html, epub, etc. This may or may not force download
- *issue* urls (i.e., <https://papiro.unizar.es/ojs/index.php/rc51-jos/issue/view/91>) pointing to a full issue of a journal
- *oai* urls (i.e., <https://papiro.unizar.es/ojs/index.php/rc51-jos/oai>) pointing to the entry page of the oai protocol

Your urls could look like these:

```{r}

url_sample <- c( 'https://publicaciones.sociales.uba.ar/index.php/psicologiasocial/article/view/2903',
          'http://fundacionmenteclara.org.ar/revista/index.php/RCA/issue/view/2018-Vol3-2',
          'http://fundacionmenteclara.org.ar/revista/index.php/RCA/article/download/43/54',
          'https://firstmonday.org/ojs/index.php/fm/article/view/9540',
          'http://imed.pub/ojs/index.php/iam/article/view/1650',
          'http://fundacionmenteclara.org.ar/revista/index.php/RCA/oai'
)

```

We'll parse them, and store the result in a dataframes (`what_type_of_urls`)

```{r}

library(ojsr) 

what_type_of_urls <- ojsr::process_urls(url_sample)
what_type_of_urls


```

## get_metadata_from_page: Scrap meta-data from OJS article_abstract pages

If you have a list of OJS *article_abstract* urls you can scrap their meta-data, by parsing the head section of their html.

In the last section we created a dataframe of OJS urls `what_type_of_urls`. In the following, we are only keeping those pointing to *article_abstract* pages. Then, we're scrapping their meta-data with `get_metadata_from_page`. 

Since this retrieves a (probably quite large and unconfortable) list, we'll then use `metadata_list_to_df` to convert it to a dataframe, in a long tidyish format: name of meta-tag, content, scheme, xml:lang, and url of the article's page. (Please note this last step may drop some meta-data, by keeping only those with a name/content pair).


```{r}

# let's keep only the urls pointing to article_abstract pages (type==article_abstract)

article_view_urls <- unlist( what_type_of_urls[which(what_type_of_urls$type=="article_abstract"),"url"] , use.names = FALSE) 
article_view_urls

# now let's scrap their meta-data ...

metadata_from_articles <- ojsr::get_metadata_from_page( article_view_urls )

# ... and convert them to a dataframe, for easier reading

metadata_from_articles_df <- ojsr::metadata_list_to_df( metadata_from_articles, article_view_urls )

str(metadata_from_articles_df)

```

This is the material you'll use for further analysis. I.e., you could plot the year of publication of your articles: 

```{r fig.height=2, fig.width=2, message=FALSE}

library(tidyverse) # we'll use dplyr and ggplot
library(lubridate) # we'll parse some years
metadata_from_articles_df %>% filter(name=="DC.Date.created") %>% # we'll be keeping only the Dublin Core creation date records ...
  transform(y=lubridate::year(content)) %>% # ... and extract the year ...
  group_by(y) %>% tally() %>%  # ... to calculate frequencies of year publication ...
    ggplot(aes(x=y,y=n)) + geom_col() + theme_light() # ... and finally plot it

```

## get_galley_urls: Retrieve galley(s) (full-content and supplementary material) urls from article_abstract pages

Another task you can perform on *article_abstract* pages is to retrieve the galley(s) (usually full-content and suplementary materials) to view/download.

In the first section we created a dataframe of OJS urls `what_type_of_urls`. In the following, we are only keeping those pointing to *article_abstract* pages. Then, we're scrapping them to retrieve the galley(s) url with `get_galley_urls`. 

```{r}

# let's keep only the urls pointing to article_abstract pages (type==article_abstract)

article_view_urls <- unlist( what_type_of_urls[which(what_type_of_urls$type=="article_abstract"),"url"] , use.names = FALSE) 
article_view_urls

# now let's scrap them to retrieve the galley(s) url ...

galleys_urls <- ojsr::get_galley_urls( article_view_urls )
galleys_urls

```

The resulting dataframe includes:

- *galley_url* = url to view/download galley file (OJS will show the galley if they have a reader plugin; otherwise it will force download)
- *galley_force_download* = url that forces download of the galley file 
- *galley_format* = the declared format of the galley (usually, either an usual format for full-content, such as pdf, epub, xml; or the title of the supplementary file)
- *url* = the url of the OJS *article_abstract* page


## get_articles_urls_from_issue: Retrieve article urls from issues pages


```{r}

# let's start by providing a bunch of urls of OJS issue pages

issues_url <- c('https://publicaciones.sociales.uba.ar/index.php/psicologiasocial/issue/view/319/showToc',
  'https://firstmonday.org/ojs/index.php/fm/issue/view/634')

# let's ???

what_type_of_urls <- ojsr::process_urls(issues_url[1])
what_type_of_urls

# let's ???

galleyLinks <- ojsr::get_articles_urls_from_issue(issues_url[1])
galleyLinks

```
