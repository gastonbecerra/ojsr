---
title: "ojsr-vignette"
author: "Gaston Becerra"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{ojsr-vignette}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup, include = FALSE}
library(knitr)
```


# Overview

The aim of this package is to aid you in several OJS tasks, such as: 

- parse OJS urls, according to routing conventions;
- crawling archives, issues, articles, galleys;
- retrieving/scraping meta-data from articles;

## Important! 

- Most of ojsr functions rely on **OJS conventions and default themes**, and therefore are most likely to fail when used on customized OJS installments (i.e., customized themes, routing improvements via .htaccess), and also on non-compliant OJS configuration or poor record keeping (i.e., when metadata has not been well-broomed by the Editor/Publisher, or OAI records do not follow standards). 
- Also, please notice that only **common OJS v1 and v2 conventions** are considered (see "OJS v3 and OJS API" below).


## About OJS

(from the OJS documentation <https://pkp.sfu.ca/ojs/>, as of Jan.2020)

Open Journal Systems (OJS) is a journal management and publishing system that has been developed by the Public Knowledge Project through its federally funded efforts to expand and improve access to research.

OJS assists with every stage of the refereed publishing process, from submissions through to online publication and indexing. Through its management systems, its finely grained indexing of research, and the context it provides for research, OJS seeks to improve both the scholarly and public quality of refereed research.

OJS is open source software made freely available to journals worldwide for the purpose of making open access publishing a viable option for more journals, as open access can increase a journalâ€™s readership as well as its contribution to the public good on a global scale (see PKP Publications).


## OJS v3 and OJS API

There is not a simpler, more reliable (less conventional!), way to access meta-data from OJS? 

Yes, there is! Since OJS v3.1+ <https://docs.pkp.sfu.ca/dev/api/ojs/3.1> a Rest API is provided, and we're positive a better R interface should use that API instead of web scraping themes and OAI records.

So, why ojsr? 

Well... According to <https://pkp.sfu.ca/ojs/ojs-usage/ojs-stats/>, as of 2019 (when v3.1+ was launched), OJS was being used by at least 10,000 journals worldwide. OJS is an excellent free publishing solution for institutions that could not publish otherwise. Presumably, most of them do not have the the technical nor financial capabilities to update constantly, so OJS v1 and v2 are expected to be around for quite some time... 



# How to use ojsr?

## process_urls: Parse urls to check what type of OJS page they point at

Let's say you have a bunch of OJS specific article urls you'd like to scrap their meta-data, or a curated list of journals you'd like to crawl completely. However, not all of them may be poiting to the *right* type of OJS page for your purposes (e.g., articles url you collected may be downloading pdf directly, so you won't be able to scrap meta-data, or the issues may be pointing at cover pages that do not include ToCs). 

`process_urls()` allows you to parse a**list of urls against OJS routing conventions** and determine what type of page are they pointing at, which parameters are invoked (e.g., article, issues, galley IDs), and generate the *(assumed to be) right* url for specific purposes, such as scraping meta-data, or scraping the list of issues for the archive of issues page. Most of the times `process_urls()` is invoked before the scraping functions (see the example code for `get_issue_urls_from_archive()`). 

Please keep in mind that `process_urls()` works with **routing conventions** (see <https://docs.pkp.sfu.ca/dev/documentation/en/architecture-routes>). The list of urls you input is not actually browsed.


```{r message=FALSE, warning=FALSE}
# let's start by loading the libraries
library(ojsr) 
library(tidyverse) # we'll use dplyr and tidy several times in the examples
```

Your urls could look like these:

```{r}
# a mix of different types of OJS pages, including a few illformed
url_sample <- c( 'https://publicaciones.sociales.uba.ar/index.php/psicologiasocial/article/view/2903',
          'http://fundacionmenteclara.org.ar/revista/index.php/RCA/issue/view/2018-Vol3-2',
          'http://fundacionmenteclara.org.ar/revista/index.php/RCA/article/download/43/54',
          'https://firstmonday.org/ojs/index.php/fm/article/view/9540',
          'http://imed.pub/ojs/index.php/iam/article/view/1650',
          'http://fundacionmenteclara.org.ar/revista/index.php/RCA/oai',
          'https://publicaciones.sociales.uba.ar/index.php/psicologiasocial/issue/view/444',
          'https://publicaciones.sociales.uba.ar/index.php/psicologiasocial/search/search?query=complejidad'
)

```

We'll parse them, and store the result in a dataframes (`what_type_of_urls`)

```{r}
what_type_of_urls <- ojsr::process_urls(url_sample)
```

The resulting dataframe contains the following columns:

1. *url* - the url you provided
2. *page* - the page class the url is pointing at (article, issue, oai, search, etc.)
3. *command* - the name of the function within the page (view, download, search, etc.)
4. *expect* - what you could do at this page; usually a combination of command + page or a variant: 
    i) *view article*: shows the abstract page, with references and links to galleys (full-content, supplementary materials) (i.e.,  <https://papiro.unizar.es/ojs/index.php/rc51-jos/article/view/1047>)
    ii) *view galley*: shows an inline reader of a galley (i.e., <https://publicaciones.sociales.uba.ar/index.php/psicologiasocial/article/view/6/2803>)
    iii) *download galley*: forces a galley download (i.e., <https://papiro.unizar.es/ojs/index.php/rc51-jos/article/download/1047/1050>)
    iv) *current issue coverpage*: redirects the current issue coverpage; may or may not include ToC and links to articles (i.e., <https://publicaciones.sociales.uba.ar/index.php/psicologiasocial/issue/current>)
    v) *view issue coverpage*: shows an issue coverpage; may or may not include ToC and links to articles (i.e., <https://publicaciones.sociales.uba.ar/index.php/psicologiasocial/issue/view/526>)
    vi) *view issue ToC*: shows the ToC of an issue (with link to articles and galleys) (i.e., <https://publicaciones.sociales.uba.ar/index.php/psicologiasocial/issue/view/526/showToC>)
    vii) *view issue archive*: shows the list of issues (i.e, <https://publicaciones.sociales.uba.ar/index.php/psicologiasocial/issue/archive>)
    viii) *oai*: shows the OAI protocol base URL (i.e., <https://publicaciones.sociales.uba.ar/index.php/psicologiasocial/oai>)
    ix) *search*: shows search form / result (i.e., <https://publicaciones.sociales.uba.ar/index.php/psicologiasocial/search/search?query=big+data>)
5. *issueId* - if an issue, its ID; otherwise ""
6. *articleId* - if an article, its ID; otherwise ""
7. *galleyId* - if a galley (full-content, supplementary materials, etc.), its ID; otherwise ""
8. *assume_article* - the *(assumed to be) right* url to scrap metadata from an article (if articleID is present, otherwise "")
9. *assume_issue* - the *(assumed to be) right* url to scrap the ToC from an issue (if issueID is present, otherwise "")
10. *assume_oai* - the *(assumed to be) right* url to the OAI records listing
11. *assume_search* - the *(assumed to be) right* url to a search result (you may contact to a query)
12. *assume_archive* - the *(assumed to be) right* url to scrap the list of issues
13. *baseUrl* - base url of the OJS, for you to do your own processing.



## get_issue_urls_from_archive: Retrieve issues urls from the archive of issues

Let's say you want to crawl a few journals in order to list their issues, so you can latter iterate through them and scrap articles' meta-data. We can use `get_issue_urls_from_archive()` for that. 

`get_issue_urls_from_archive()` requires a list OJS urls pointing at the archive of issues of each journal (e.g., <https://papiro.unizar.es/ojs/index.php/rc51-jos/issue/archive>). If you are unsure about your url list, you can first pre-process it with `process_urls()` and use the  *assumed-to-be-right* url instead (**$assume_archive**).

```{r}
# let's start by providing a mix url list of argentinian social psychology journals 
socPsy_urls <- c( 
  'http://sportsem.uv.es/j_sports_and_em/index.php/rips/', # home page of journal, including ToC of current issue
  'https://dspace.palermo.edu/ojs/index.php/psicodebate/issue/archive', # points at the archive of issues
  'https://publicaciones.sociales.uba.ar/index.php/psicologiasocial/article/view/2903' # points at an article 
)

# we'll use process_urls to retrieve the assumed url for archive of issues (what_type_of_urls$assume_archive);
# we'll pass that as the url list to scrap 
what_type_of_urls <- ojsr::process_urls(socPsy_urls)
issues <- ojsr::get_issue_urls_from_archive(what_type_of_urls$assume_archive)

# result is in a long-format dataframe, containing the provided url in "url" (col1), and the issues url in "links" (col2)
glimpse(issues)
```

### Available methods
- **scrap_by_href_convention** (default): checks the urls in the links for the expected convention "/issue/view"



## get_article_urls_from_issue: Retrieve articles urls from issue pages

Let's say you want to crawl particular issues in order to list their articles, so you can latter iterate through them and scrap their meta-data. We can use `get_article_urls_from_issue()` for that. 

`get_article_urls_from_issue()` requires a list OJS urls pointing at the issues of each journal (e.g., <https://publicaciones.sociales.uba.ar/index.php/psicologiasocial/issue/view/319/showToc>). Issues in OJS may or may not show a cover page, and show or not show the table of contents (this is where the links to the article reside). Usually, you can force the display of the ToC by adding "/ShowToC" to the url. You could pre-process your url list with `process_urls()` and use the  *assumed-to-be-right* url instead (**$assume_issue**).

```{r}
# let's start by providing the initial issues from a few argentinian social psychology journals 
socPsy_issues_url <- c( 
  'http://sportsem.uv.es/j_sports_and_em/index.php/rips/issue/view/5', # includes ToC
  'https://dspace.palermo.edu/ojs/index.php/psicodebate/issue/view/Psicodebate%201', # includes ToC
  'https://publicaciones.sociales.uba.ar/index.php/psicologiasocial/issue/view/31' # does not include ToC
)

# we'll use process_urls to retrieve the assumed url for issues including the ToC (what_type_of_urls$assume_issue);
# we'll pass that as the url list to scrap 
what_type_of_urls <- ojsr::process_urls(socPsy_issues_url)
articles <- ojsr::get_article_urls_from_issue(what_type_of_urls$assume_issue)

# result is in a long-format dataframe, containing the provided url in "url" (col1), and the articles url in "links" (col2)
glimpse(articles)
```

### Available methods

- **scrap_by_href_convention_no_classes** (default): checks the urls in the links for the expected convention "/article/view" and then filters only those with no "pdf" or "file" classes (usually pointing to a galley, such as, full-content, secondary reading formats, and/or supplementary files)
- **scrap_by_href_convention**: checks the urls in the links for the expected convention "/article/view"; if the issue points both to the articles and their galleys, these may result in more than one link to an article.


## get_galley_urls_from_article: Retrieve articles urls from article pages

If you have a list of articles urls, you may retrieve the galley links (full-content pdfs and other reading formats, and also any supplementary files) via  `get_galley_urls_from_article()`.

Galleys are usually linked from other type of pages too, other than articles, like some issues ToC, or other galleys inline readers. However, the safest place to search for them is the "view article" page. You could pre-process your url list with `process_urls()` and use the  *assumed-to-be-right* url instead (**$assume_article**).

```{r}
# let's say we want to get the galleys from the articles of a complete issue
# we should start by crawling the issue, then crawling the resulting articles
# we'll use process_urls only in the first (crawling issue) and pass directly the articles urls to get the galleys
issue <- 'https://dspace.palermo.edu/ojs/index.php/psicodebate/issue/view/41'
what_type_of_url <- ojsr::process_urls(issue)
articles <- ojsr::get_article_urls_from_issue(what_type_of_url$assume_issue)
galleys <- ojsr::get_galley_urls_from_article(articles$links)
# result is in a long-format dataframe, containing the provided url in "url" (col1), and the galley link in "links" (col2),
# the galley format in "format" (col3) and the url to force download in "force" (col4)
# you may filter some of these, and then pass this vector to a download function
# i.e., using dplyr's filter on format, and download.file:
# url_download <- galleys %>% filter(format=="pdf") %>% select(force) %>% unlist(use.names = FALSE)
# for (i in 1:length(url_download)){ 
#    download.file( url_download[i], mode = 'wb' , destfile = paste0(getwd(),"/",i,".pdf")) 
# }
```

### Available methods

- **scrap_by_class_convention** (default): checks the classes in the links and retrieves where "file" or "obj_galley_link" are found.

## get_metadata_from_article: Retrieve metadata from article pages

If you have a list of articles urls, you may retrieve the galley links (full-content pdfs and other reading formats, and also any supplementary files) via  `get_galley_urls_from_article()`.

Galleys are usually linked from other type of pages too, other than articles, like some issues ToC, or other galleys inline readers. However, the safest place to search for them is the "view article" page. You could pre-process your url list with `process_urls()` and use the  *assumed-to-be-right* url instead (**$assume_article**).

```{r}
# let's say we want to get the meta data from the articles of a complete issue
# we should start by crawling the issue, then scraping the resulting articles
# we'll use process_urls() only in the first (crawling issue) and pass directly the articles' urls to the metadata scraper
issue <- 'https://dspace.palermo.edu/ojs/index.php/psicodebate/issue/view/41'
what_type_of_url <- ojsr::process_urls(issue)
articles <- ojsr::get_article_urls_from_issue(what_type_of_url$assume_issue)
metadata <- ojsr::get_metadata_from_article(articles$links, verbose = TRUE)
# result is in a long-format dataframe, containing the provided url in "url" (col1), xxx,

# # you may xxx
# # i.e., using dplyr's filter on format, and download.file
# url_download <- galleys %>% filter(format=="pdf") %>% select(force) %>% unlist(use.names = FALSE)
# for (i in 1:length(url_download)){ 
#    download.file( url_download[i], mode = 'wb' , destfile = paste0(getwd(),"/",i,".pdf")) 
# }
```

### Available methods

- **scrap_by_class_convention** (default): checks the classes in the links and retrieves where "file" or "obj_galley_link" are found.

